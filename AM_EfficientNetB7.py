# -*- coding: utf-8 -*-
"""AM_efficientnetB7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P94tgbJwjRfiFS1cUqMtDGJ3lgG8ppsg
"""

#! pip install -U git+https://github.com/qubvel/efficientnet

from tensorflow.keras import backend as K
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Flatten, Dense, Dropout
from tensorflow.keras.applications.xception import Xception, preprocess_input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
#from keras.callbacks.callbacks import ReduceLROnPlateau, ModelCheckpoint
from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint

from keras.applications.imagenet_utils import decode_predictions

import efficientnet.tfkeras as efn 
from efficientnet.keras import center_crop_and_resize, preprocess_input

IM_SIZE = (224,224)
diseases_classes=['Asthma',  'Bronchiectasis', 'Bronchiolitis', 'COPD', 'Healthy', 'LRTI', 'Pneumonia', 
              'URTI']
#'0Parus', '1Passe', '2Lusci', '3Phoen', '4Erith', '5Phoen', '6Sitta', '7Alaud', '8Phyll', '9Turdu',
#               '10Phyll', '11Fring', '12Sturn', '13Ember', '14Colum', '15Trogl', '16Cardu', '17Chlor', '18Turdu']
train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,  
                                   width_shift_range=0.2,
                                   height_shift_range=0.2,
                                   shear_range=0.2,
                                   zoom_range=0.1,
                                   fill_mode='nearest')

train_batches = train_datagen.flow_from_directory(r"E:\\TFG\\Raul\\final\\train\\",
                                                  classes=diseases_classes,
                                                  target_size=IM_SIZE,
                                                  class_mode='categorical', shuffle=True,
                                                  batch_size=8)

valid_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)
valid_batches = valid_datagen.flow_from_directory(r"E:\\TFG\\Raul\\final\\valid\\",
                                                  classes=diseases_classes,
                                                  target_size=IM_SIZE,
                                                  class_mode='categorical', shuffle=False,
                                                  batch_size=8)

# show class indices
print('****************')
for cls, idx in train_batches.class_indices.items():
    print('Class nr ',idx,' -> ', cls)
print('****************')

ModelCheck = ModelCheckpoint(r"E:\\TFG\\Raul\\models\\efficientnet-b7_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5", monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=True, mode='auto')
ReduceLR = ReduceLROnPlateau(monitor='val_loss',
							 factor=0.2,
							 patience=2,
							 mode='auto',
							 lr=0.0001)

test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)
test_batches = test_datagen.flow_from_directory(r"E:\\TFG\\Raul\\final\\test\\",
                                                  classes=diseases_classes,
                                                  target_size=IM_SIZE,
                                                  class_mode='categorical', shuffle=False,
                                                  batch_size=8)
# show class indices
# Provar EfficientNetB7

net = efn.EfficientNetB7(include_top=False,
                         weights='imagenet',
                         input_tensor=None,
                         input_shape=(224,224,3))
x = net.output
x = Flatten()(x)
x = Dropout(0.4)(x)
output_layer = Dense(8, activation='softmax', name='softmax')(x)
net_final = Model(inputs=net.input, outputs=output_layer)
for layer in net_final.layers[:9]:
    layer.trainable = False
for layer in net_final.layers[9:]:
    layer.trainable = True
net_final.compile(optimizer=Adam(),
                  loss='categorical_crossentropy', metrics=['accuracy'])
print(net_final.summary())

# load weights into new model
#net_final.load_weights("../models/AM_efficientnet_30classes.h5")
#print("Loaded model from disk")

# evaluate loaded model on test data
#net_final.compile(optimizer=Adam(learning_rate=5e-5),
#                  loss='categorical_crossentropy', metrics=['accuracy'])
#print(test_batches.classes)
#score = net_final.evaluate(test_batches.classes, verbose=0)
#print("%s: %.2f%%" % (net_final.metrics_names[1], score[1]*100))

import numpy as np

from sklearn.utils import class_weight
class_weights = class_weight.compute_class_weight(
                class_weight='balanced',
                classes=np.unique(train_batches.classes), 
                y=train_batches.classes)
class_weights
print(class_weights)
#print(train_batches.classes)
# train the model
#net_final.fit_generator(train_batches,
#                        validation_data = valid_batches,
#                        epochs = 20,
#                        steps_per_epoch= 1000,
#                        class_weight=class_weights, callbacks=[ModelCheck,ReduceLR])

#r = model.fit_generator(train_batches,
#                        validation_data=valid_batches,
#                        epochs=20,
#                        steps_per_epoch=1000, #len(training_set),
#                        class_weight=class_weights,
#                        validation_steps=len(valid_batches),
#                        callbacks=[ModelCheck,ReduceLR] )

r = net_final.fit(train_batches,
                  #validation_batches,
                  steps_per_epoch=92,
                  epochs=10, 
                  validation_data=valid_batches, 
                  validation_steps=30,
#                  callbacks=[ReduceLR, ModelCheck],
                  #[ReduceLR, ModelCheck],
#              shuffle=False, 
                  class_weight=dict(enumerate(class_weights))
#    validation_steps=None, validation_freq=1, max_queue_size=10, workers=1,
#    use_multiprocessing=False, **kwargs
)

# save trained weights
net_final.save(r"E:\\TFG\\Raul\\models\\AM_efficientnetB7_diseasesclasses.h5")

#test2_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,  
#                                   width_shift_range=0.2,
#                                   height_shift_range=0.2,
#                                   shear_range=0.2,
#                                   zoom_range=0.1,
#                                   fill_mode='nearest')
#test2_batches = test2_datagen.flow_from_directory('../data/final/test',
#                                                  classes=birds_classes,
#                                                  target_size=IM_SIZE,
#                                                  class_mode='categorical', shuffle=False,
#                                                  batch_size=16)
print('****************')
print(len(test_batches))
for cls, idx in test_batches.class_indices.items():
    print('Class nr ',idx,' -> ', cls)
print('****************')

# Found 85738 images belonging to 30 classes.
y_pred = net_final.predict(test_batches, 85738 // 30)
#y_pred0 =(y_pred>0.5)
#list(y_pred0)

#print(test_batches.summary())
#print(y_pred.summary())
#cm = confusion_matrix(np.asarray(test_batches), np.asarray(y_pred0))
#print(cm)

# evaluate the model

y_pred1 = np.argmax(y_pred, axis=1)

#loss, accuracy, f1_score, precision, recall = net_final.evaluate(test_batches.classes, y_pred1, verbose=0)
#print(loss, accuracy, f1_score, precision, recall)

# Print f1, precision, and recall scores
#print(precision_score(test_batches, y_pred1 , average="macro"))
#print(recall_score(test_batches, y_pred1 , average="macro"))
#print(f1_score(test_batches, y_pred1 , average="macro"))

from sklearn.metrics import classification_report, confusion_matrix
#print(classification_report(test_batches, y_pred1))

print('Confusion Matrix')
print(confusion_matrix(test_batches.classes, y_pred1))
print('Classification Report')
target_names=['Asthma',  'Bronchiectasis', 'Bronchiolitis', 'COPD', 'Healthy', 'LRTI', 'Pneumonia', 
              'URTI']

print(classification_report(test_batches.classes, y_pred1, target_names=target_names))
# -*- coding: utf-8 -*-
"""AM_efficientnetB7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P94tgbJwjRfiFS1cUqMtDGJ3lgG8ppsg
"""

#! pip install -U git+https://github.com/qubvel/efficientnet

from tensorflow.keras import backend as K
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Flatten, Dense, Dropout
from tensorflow.keras.applications.xception import Xception, preprocess_input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
#from keras.callbacks.callbacks import ReduceLROnPlateau, ModelCheckpoint
from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint

from keras.applications.imagenet_utils import decode_predictions

import efficientnet.tfkeras as efn 
from efficientnet.keras import center_crop_and_resize, preprocess_input

IM_SIZE = (224,224)
diseases_classes=['Asthma',  'Bronchiectasis', 'Bronchiolitis', 'COPD', 'Healthy', 'LRTI', 'Pneumonia', 
              'URTI']
#'0Parus', '1Passe', '2Lusci', '3Phoen', '4Erith', '5Phoen', '6Sitta', '7Alaud', '8Phyll', '9Turdu',
#               '10Phyll', '11Fring', '12Sturn', '13Ember', '14Colum', '15Trogl', '16Cardu', '17Chlor', '18Turdu']
train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,  
                                   width_shift_range=0.2,
                                   height_shift_range=0.2,
                                   shear_range=0.2,
                                   zoom_range=0.1,
                                   fill_mode='nearest')

train_batches = train_datagen.flow_from_directory(r"E:\\TFG\\Raul\\final\\train\\",
                                                  classes=diseases_classes,
                                                  target_size=IM_SIZE,
                                                  class_mode='categorical', shuffle=True,
                                                  batch_size=8)

valid_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)
valid_batches = valid_datagen.flow_from_directory(r"E:\\TFG\\Raul\\final\\valid\\",
                                                  classes=diseases_classes,
                                                  target_size=IM_SIZE,
                                                  class_mode='categorical', shuffle=False,
                                                  batch_size=8)

# show class indices
print('****************')
for cls, idx in train_batches.class_indices.items():
    print('Class nr ',idx,' -> ', cls)
print('****************')

ModelCheck = ModelCheckpoint(r"E:\\TFG\\Raul\\models\\efficientnet-b7_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5", monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=True, mode='auto')
ReduceLR = ReduceLROnPlateau(monitor='val_loss',
							 factor=0.2,
							 patience=2,
							 mode='auto',
							 lr=0.0001)

test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)
test_batches = test_datagen.flow_from_directory(r"E:\\TFG\\Raul\\final\\test\\",
                                                  classes=diseases_classes,
                                                  target_size=IM_SIZE,
                                                  class_mode='categorical', shuffle=False,
                                                  batch_size=8)
# show class indices
# Provar EfficientNetB7

net = efn.EfficientNetB7(include_top=False,
                         weights='imagenet',
                         input_tensor=None,
                         input_shape=(224,224,3))
x = net.output
x = Flatten()(x)
x = Dropout(0.4)(x)
output_layer = Dense(8, activation='softmax', name='softmax')(x)
net_final = Model(inputs=net.input, outputs=output_layer)
for layer in net_final.layers[:9]:
    layer.trainable = False
for layer in net_final.layers[9:]:
    layer.trainable = True
net_final.compile(optimizer=Adam(),
                  loss='categorical_crossentropy', metrics=['accuracy'])
print(net_final.summary())

# load weights into new model
#net_final.load_weights("../models/AM_efficientnet_30classes.h5")
#print("Loaded model from disk")

# evaluate loaded model on test data
#net_final.compile(optimizer=Adam(learning_rate=5e-5),
#                  loss='categorical_crossentropy', metrics=['accuracy'])
#print(test_batches.classes)
#score = net_final.evaluate(test_batches.classes, verbose=0)
#print("%s: %.2f%%" % (net_final.metrics_names[1], score[1]*100))

import numpy as np

from sklearn.utils import class_weight
class_weights = class_weight.compute_class_weight(
                class_weight='balanced',
                classes=np.unique(train_batches.classes), 
                y=train_batches.classes)
class_weights
print(class_weights)
#print(train_batches.classes)
# train the model
#net_final.fit_generator(train_batches,
#                        validation_data = valid_batches,
#                        epochs = 20,
#                        steps_per_epoch= 1000,
#                        class_weight=class_weights, callbacks=[ModelCheck,ReduceLR])

#r = model.fit_generator(train_batches,
#                        validation_data=valid_batches,
#                        epochs=20,
#                        steps_per_epoch=1000, #len(training_set),
#                        class_weight=class_weights,
#                        validation_steps=len(valid_batches),
#                        callbacks=[ModelCheck,ReduceLR] )

r = net_final.fit(train_batches,
                  #validation_batches,
                  steps_per_epoch=92,
                  epochs=10, 
                  validation_data=valid_batches, 
                  validation_steps=30,
#                  callbacks=[ReduceLR, ModelCheck],
                  #[ReduceLR, ModelCheck],
#              shuffle=False, 
                  class_weight=dict(enumerate(class_weights))
#    validation_steps=None, validation_freq=1, max_queue_size=10, workers=1,
#    use_multiprocessing=False, **kwargs
)

# save trained weights
net_final.save(r"E:\\TFG\\Raul\\models\\AM_efficientnetB7_diseasesclasses.h5")

#test2_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,  
#                                   width_shift_range=0.2,
#                                   height_shift_range=0.2,
#                                   shear_range=0.2,
#                                   zoom_range=0.1,
#                                   fill_mode='nearest')
#test2_batches = test2_datagen.flow_from_directory('../data/final/test',
#                                                  classes=birds_classes,
#                                                  target_size=IM_SIZE,
#                                                  class_mode='categorical', shuffle=False,
#                                                  batch_size=16)
print('****************')
print(len(test_batches))
for cls, idx in test_batches.class_indices.items():
    print('Class nr ',idx,' -> ', cls)
print('****************')

# Found 85738 images belonging to 30 classes.
y_pred = net_final.predict(test_batches, 85738 // 30)
#y_pred0 =(y_pred>0.5)
#list(y_pred0)

#print(test_batches.summary())
#print(y_pred.summary())
#cm = confusion_matrix(np.asarray(test_batches), np.asarray(y_pred0))
#print(cm)

# evaluate the model

y_pred1 = np.argmax(y_pred, axis=1)

#loss, accuracy, f1_score, precision, recall = net_final.evaluate(test_batches.classes, y_pred1, verbose=0)
#print(loss, accuracy, f1_score, precision, recall)

# Print f1, precision, and recall scores
#print(precision_score(test_batches, y_pred1 , average="macro"))
#print(recall_score(test_batches, y_pred1 , average="macro"))
#print(f1_score(test_batches, y_pred1 , average="macro"))

from sklearn.metrics import classification_report, confusion_matrix
#print(classification_report(test_batches, y_pred1))

print('Confusion Matrix')
print(confusion_matrix(test_batches.classes, y_pred1))
print('Classification Report')
target_names=['Asthma',  'Bronchiectasis', 'Bronchiolitis', 'COPD', 'Healthy', 'LRTI', 'Pneumonia', 
              'URTI']

print(classification_report(test_batches.classes, y_pred1, target_names=target_names))
# -*- coding: utf-8 -*-
"""AM_efficientnetB7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P94tgbJwjRfiFS1cUqMtDGJ3lgG8ppsg
"""

#! pip install -U git+https://github.com/qubvel/efficientnet

from tensorflow.keras import backend as K
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Flatten, Dense, Dropout
from tensorflow.keras.applications.xception import Xception, preprocess_input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
#from keras.callbacks.callbacks import ReduceLROnPlateau, ModelCheckpoint
from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint

from keras.applications.imagenet_utils import decode_predictions

import efficientnet.tfkeras as efn 
from efficientnet.keras import center_crop_and_resize, preprocess_input

IM_SIZE = (224,224)
diseases_classes=['Asthma',  'Bronchiectasis', 'Bronchiolitis', 'COPD', 'Healthy', 'LRTI', 'Pneumonia', 
              'URTI']
#'0Parus', '1Passe', '2Lusci', '3Phoen', '4Erith', '5Phoen', '6Sitta', '7Alaud', '8Phyll', '9Turdu',
#               '10Phyll', '11Fring', '12Sturn', '13Ember', '14Colum', '15Trogl', '16Cardu', '17Chlor', '18Turdu']
train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,  
                                   width_shift_range=0.2,
                                   height_shift_range=0.2,
                                   shear_range=0.2,
                                   zoom_range=0.1,
                                   fill_mode='nearest')

train_batches = train_datagen.flow_from_directory(r"E:\\TFG\\Raul\\final\\train\\",
                                                  classes=diseases_classes,
                                                  target_size=IM_SIZE,
                                                  class_mode='categorical', shuffle=True,
                                                  batch_size=8)

valid_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)
valid_batches = valid_datagen.flow_from_directory(r"E:\\TFG\\Raul\\final\\valid\\",
                                                  classes=diseases_classes,
                                                  target_size=IM_SIZE,
                                                  class_mode='categorical', shuffle=False,
                                                  batch_size=8)

# show class indices
print('****************')
for cls, idx in train_batches.class_indices.items():
    print('Class nr ',idx,' -> ', cls)
print('****************')

ModelCheck = ModelCheckpoint(r"E:\\TFG\\Raul\\models\\efficientnet-b7_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5", monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=True, mode='auto')
ReduceLR = ReduceLROnPlateau(monitor='val_loss',
							 factor=0.2,
							 patience=2,
							 mode='auto',
							 lr=0.0001)

test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)
test_batches = test_datagen.flow_from_directory(r"E:\\TFG\\Raul\\final\\test\\",
                                                  classes=diseases_classes,
                                                  target_size=IM_SIZE,
                                                  class_mode='categorical', shuffle=False,
                                                  batch_size=8)
# show class indices
# Provar EfficientNetB7

net = efn.EfficientNetB7(include_top=False,
                         weights='imagenet',
                         input_tensor=None,
                         input_shape=(224,224,3))
x = net.output
x = Flatten()(x)
x = Dropout(0.4)(x)
output_layer = Dense(8, activation='softmax', name='softmax')(x)
net_final = Model(inputs=net.input, outputs=output_layer)
for layer in net_final.layers[:9]:
    layer.trainable = False
for layer in net_final.layers[9:]:
    layer.trainable = True
net_final.compile(optimizer=Adam(),
                  loss='categorical_crossentropy', metrics=['accuracy'])
print(net_final.summary())

# load weights into new model
#net_final.load_weights("../models/AM_efficientnet_30classes.h5")
#print("Loaded model from disk")

# evaluate loaded model on test data
#net_final.compile(optimizer=Adam(learning_rate=5e-5),
#                  loss='categorical_crossentropy', metrics=['accuracy'])
#print(test_batches.classes)
#score = net_final.evaluate(test_batches.classes, verbose=0)
#print("%s: %.2f%%" % (net_final.metrics_names[1], score[1]*100))

import numpy as np

from sklearn.utils import class_weight
class_weights = class_weight.compute_class_weight(
                class_weight='balanced',
                classes=np.unique(train_batches.classes), 
                y=train_batches.classes)
class_weights
print(class_weights)
#print(train_batches.classes)
# train the model
#net_final.fit_generator(train_batches,
#                        validation_data = valid_batches,
#                        epochs = 20,
#                        steps_per_epoch= 1000,
#                        class_weight=class_weights, callbacks=[ModelCheck,ReduceLR])

#r = model.fit_generator(train_batches,
#                        validation_data=valid_batches,
#                        epochs=20,
#                        steps_per_epoch=1000, #len(training_set),
#                        class_weight=class_weights,
#                        validation_steps=len(valid_batches),
#                        callbacks=[ModelCheck,ReduceLR] )

r = net_final.fit(train_batches,
                  #validation_batches,
                  steps_per_epoch=92,
                  epochs=10, 
                  validation_data=valid_batches, 
                  validation_steps=30,
#                  callbacks=[ReduceLR, ModelCheck],
                  #[ReduceLR, ModelCheck],
#              shuffle=False, 
                  class_weight=dict(enumerate(class_weights))
#    validation_steps=None, validation_freq=1, max_queue_size=10, workers=1,
#    use_multiprocessing=False, **kwargs
)

# save trained weights
net_final.save(r"E:\\TFG\\Raul\\models\\AM_efficientnetB7_diseasesclasses.h5")

#test2_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,  
#                                   width_shift_range=0.2,
#                                   height_shift_range=0.2,
#                                   shear_range=0.2,
#                                   zoom_range=0.1,
#                                   fill_mode='nearest')
#test2_batches = test2_datagen.flow_from_directory('../data/final/test',
#                                                  classes=birds_classes,
#                                                  target_size=IM_SIZE,
#                                                  class_mode='categorical', shuffle=False,
#                                                  batch_size=16)
print('****************')
print(len(test_batches))
for cls, idx in test_batches.class_indices.items():
    print('Class nr ',idx,' -> ', cls)
print('****************')

# Found 85738 images belonging to 30 classes.
y_pred = net_final.predict(test_batches, 85738 // 30)
#y_pred0 =(y_pred>0.5)
#list(y_pred0)

#print(test_batches.summary())
#print(y_pred.summary())
#cm = confusion_matrix(np.asarray(test_batches), np.asarray(y_pred0))
#print(cm)

# evaluate the model

y_pred1 = np.argmax(y_pred, axis=1)

#loss, accuracy, f1_score, precision, recall = net_final.evaluate(test_batches.classes, y_pred1, verbose=0)
#print(loss, accuracy, f1_score, precision, recall)

# Print f1, precision, and recall scores
#print(precision_score(test_batches, y_pred1 , average="macro"))
#print(recall_score(test_batches, y_pred1 , average="macro"))
#print(f1_score(test_batches, y_pred1 , average="macro"))

from sklearn.metrics import classification_report, confusion_matrix
#print(classification_report(test_batches, y_pred1))

print('Confusion Matrix')
print(confusion_matrix(test_batches.classes, y_pred1))
print('Classification Report')
target_names=['Asthma',  'Bronchiectasis', 'Bronchiolitis', 'COPD', 'Healthy', 'LRTI', 'Pneumonia', 
              'URTI']

print(classification_report(test_batches.classes, y_pred1, target_names=target_names))
# -*- coding: utf-8 -*-
"""AM_efficientnetB7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P94tgbJwjRfiFS1cUqMtDGJ3lgG8ppsg
"""

#! pip install -U git+https://github.com/qubvel/efficientnet

from tensorflow.keras import backend as K
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Flatten, Dense, Dropout
from tensorflow.keras.applications.xception import Xception, preprocess_input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
#from keras.callbacks.callbacks import ReduceLROnPlateau, ModelCheckpoint
from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint

from keras.applications.imagenet_utils import decode_predictions

import efficientnet.tfkeras as efn 
from efficientnet.keras import center_crop_and_resize, preprocess_input

IM_SIZE = (224,224)
diseases_classes=['Asthma',  'Bronchiectasis', 'Bronchiolitis', 'COPD', 'Healthy', 'LRTI', 'Pneumonia', 
              'URTI']
#'0Parus', '1Passe', '2Lusci', '3Phoen', '4Erith', '5Phoen', '6Sitta', '7Alaud', '8Phyll', '9Turdu',
#               '10Phyll', '11Fring', '12Sturn', '13Ember', '14Colum', '15Trogl', '16Cardu', '17Chlor', '18Turdu']
train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,  
                                   width_shift_range=0.2,
                                   height_shift_range=0.2,
                                   shear_range=0.2,
                                   zoom_range=0.1,
                                   fill_mode='nearest')

train_batches = train_datagen.flow_from_directory(r"E:\\TFG\\Raul\\final\\train\\",
                                                  classes=diseases_classes,
                                                  target_size=IM_SIZE,
                                                  class_mode='categorical', shuffle=True,
                                                  batch_size=8)

valid_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)
valid_batches = valid_datagen.flow_from_directory(r"E:\\TFG\\Raul\\final\\valid\\",
                                                  classes=diseases_classes,
                                                  target_size=IM_SIZE,
                                                  class_mode='categorical', shuffle=False,
                                                  batch_size=8)

# show class indices
print('****************')
for cls, idx in train_batches.class_indices.items():
    print('Class nr ',idx,' -> ', cls)
print('****************')

ModelCheck = ModelCheckpoint(r"E:\\TFG\\Raul\\models\\efficientnet-b7_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5", monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=True, mode='auto')
ReduceLR = ReduceLROnPlateau(monitor='val_loss',
							 factor=0.2,
							 patience=2,
							 mode='auto',
							 lr=0.0001)

test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)
test_batches = test_datagen.flow_from_directory(r"E:\\TFG\\Raul\\final\\test\\",
                                                  classes=diseases_classes,
                                                  target_size=IM_SIZE,
                                                  class_mode='categorical', shuffle=False,
                                                  batch_size=8)
# show class indices
# Provar EfficientNetB7

net = efn.EfficientNetB7(include_top=False,
                         weights='imagenet',
                         input_tensor=None,
                         input_shape=(224,224,3))
x = net.output
x = Flatten()(x)
x = Dropout(0.4)(x)
output_layer = Dense(8, activation='softmax', name='softmax')(x)
net_final = Model(inputs=net.input, outputs=output_layer)
for layer in net_final.layers[:9]:
    layer.trainable = False
for layer in net_final.layers[9:]:
    layer.trainable = True
net_final.compile(optimizer=Adam(),
                  loss='categorical_crossentropy', metrics=['accuracy'])
print(net_final.summary())

# load weights into new model
#net_final.load_weights("../models/AM_efficientnet_30classes.h5")
#print("Loaded model from disk")

# evaluate loaded model on test data
#net_final.compile(optimizer=Adam(learning_rate=5e-5),
#                  loss='categorical_crossentropy', metrics=['accuracy'])
#print(test_batches.classes)
#score = net_final.evaluate(test_batches.classes, verbose=0)
#print("%s: %.2f%%" % (net_final.metrics_names[1], score[1]*100))

import numpy as np

from sklearn.utils import class_weight
class_weights = class_weight.compute_class_weight(
                class_weight='balanced',
                classes=np.unique(train_batches.classes), 
                y=train_batches.classes)
class_weights
print(class_weights)
#print(train_batches.classes)
# train the model
#net_final.fit_generator(train_batches,
#                        validation_data = valid_batches,
#                        epochs = 20,
#                        steps_per_epoch= 1000,
#                        class_weight=class_weights, callbacks=[ModelCheck,ReduceLR])

#r = model.fit_generator(train_batches,
#                        validation_data=valid_batches,
#                        epochs=20,
#                        steps_per_epoch=1000, #len(training_set),
#                        class_weight=class_weights,
#                        validation_steps=len(valid_batches),
#                        callbacks=[ModelCheck,ReduceLR] )

r = net_final.fit(train_batches,
                  #validation_batches,
                  steps_per_epoch=92,
                  epochs=10, 
                  validation_data=valid_batches, 
                  validation_steps=30,
#                  callbacks=[ReduceLR, ModelCheck],
                  #[ReduceLR, ModelCheck],
#              shuffle=False, 
                  class_weight=dict(enumerate(class_weights))
#    validation_steps=None, validation_freq=1, max_queue_size=10, workers=1,
#    use_multiprocessing=False, **kwargs
)

# save trained weights
net_final.save(r"E:\\TFG\\Raul\\models\\AM_efficientnetB7_diseasesclasses.h5")

#test2_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,  
#                                   width_shift_range=0.2,
#                                   height_shift_range=0.2,
#                                   shear_range=0.2,
#                                   zoom_range=0.1,
#                                   fill_mode='nearest')
#test2_batches = test2_datagen.flow_from_directory('../data/final/test',
#                                                  classes=birds_classes,
#                                                  target_size=IM_SIZE,
#                                                  class_mode='categorical', shuffle=False,
#                                                  batch_size=16)
print('****************')
print(len(test_batches))
for cls, idx in test_batches.class_indices.items():
    print('Class nr ',idx,' -> ', cls)
print('****************')

# Found 85738 images belonging to 30 classes.
y_pred = net_final.predict(test_batches, 85738 // 30)
#y_pred0 =(y_pred>0.5)
#list(y_pred0)

#print(test_batches.summary())
#print(y_pred.summary())
#cm = confusion_matrix(np.asarray(test_batches), np.asarray(y_pred0))
#print(cm)

# evaluate the model

y_pred1 = np.argmax(y_pred, axis=1)

#loss, accuracy, f1_score, precision, recall = net_final.evaluate(test_batches.classes, y_pred1, verbose=0)
#print(loss, accuracy, f1_score, precision, recall)

# Print f1, precision, and recall scores
#print(precision_score(test_batches, y_pred1 , average="macro"))
#print(recall_score(test_batches, y_pred1 , average="macro"))
#print(f1_score(test_batches, y_pred1 , average="macro"))

from sklearn.metrics import classification_report, confusion_matrix
#print(classification_report(test_batches, y_pred1))

print('Confusion Matrix')
print(confusion_matrix(test_batches.classes, y_pred1))
print('Classification Report')
target_names=['Asthma',  'Bronchiectasis', 'Bronchiolitis', 'COPD', 'Healthy', 'LRTI', 'Pneumonia', 
              'URTI']

print(classification_report(test_batches.classes, y_pred1, target_names=target_names))
