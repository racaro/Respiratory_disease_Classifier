
"""AM_evaluate.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hrEHZQrpjzxujGAa7OT7ojjt7orMsnhm
"""

#from google.colab import drive
#rive.mount('/content/drive')
# Install the python version

'''
!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 1
!sudo update-alternatives --config python3
!python3 --version
'''

path_base = r"E:\\TFG\\Raul\\"
#"/home/raul/Descargas/Clasificador_Enfermo-20220829T191516Z-001/Clasificador_Enfermo/"

#!pip install tensorflow

from tensorflow.keras import backend as K
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Flatten, Dense, Dropout
from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import LearningRateScheduler
import h5py

'''
{0: 'Asthma',
 1: 'Bronchiectasis',
 2: 'Bronchiolitis',
 3: 'COPD',
 4: 'Healthy',
 5: 'LRTI',
 6: 'Pneumonia',
 7: 'URTI'},
'''
IM_SIZE=(224,224)
#diseases_classes=['Asthma',  'Bronchiectasis', 'Bronchiolitis', 'COPD', 'Healthy', 'LRTI', 'Pneumonia', 
#              'URTI']

diseases_classes=['Asthma',  'Bronchiectasis', 'Bronchiolitis', 'COPD', 'Healthy', 'LRTI', 'Pneumonia', 
              'URTI']


train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,  
                                   width_shift_range=0.2,
                                   height_shift_range=0.2,
                                   shear_range=0.2,
                                   zoom_range=0.1,
                                   fill_mode='nearest')

#Respiratory_Sound_Database-20220829T195035Z-001\\Respiratory_Sound_Database\\Respiratory_Sound_Database\\Respiratory_Sound_Database\\Enfermedades\\",
train_batches = train_datagen.flow_from_directory(r"E:\\TFG\\Raul\\final\\train\\",
                                                  classes=diseases_classes,
                                                  target_size=IM_SIZE,
                                                  class_mode='categorical', shuffle=True,
                                                  batch_size=4)

valid_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)
#Respiratory_Sound_Database-20220829T195035Z-001\\Respiratory_Sound_Database\\Respiratory_Sound_Database\\Respiratory_Sound_Database\\Enfermedades\\",
#"/home/raul/Descargas/Respiratory_Sound_Database-20220829T195035Z-001/Respiratory_Sound_Database/Respiratory_Sound_Database/Respiratory_Sound_Database/Enfermedades",

valid_batches = valid_datagen.flow_from_directory(r"E:\\TFG\\Raul\\final\\valid\\",
                                                  classes=diseases_classes,
                                                  target_size=IM_SIZE,
                                                  class_mode='categorical', shuffle=False,
                                                  batch_size=4)

# show class indices
print('****************')
for cls, idx in train_batches.class_indices.items():
    print('Class nr ',idx,' -> ', cls)
print('****************')


net = MobileNetV2(include_top=False,
                        weights='imagenet',
                        input_tensor=None,
                        input_shape=(224,224,3))
x = net.output
x = Flatten()(x)
x = Dropout(0.5)(x)
output_layer = Dense(8, activation='softmax', name='softmax')(x)
loaded_model = Model(inputs=net.input, outputs=output_layer)

#for layer in net.layers[:9]:
#    layer.trainable = False
#for layer in net.layers[9:]:
#    layer.trainable = True

# load weights into new model
#loaded_model.load_weights("../models/AM_mobilenetV2.h5")
#print("Loaded model from disk")

# evaluate loaded model on test data
loaded_model.compile(optimizer=Adam(lr=3e-5),
                  loss='categorical_crossentropy', metrics=['accuracy'])
print(loaded_model.summary())

#score = loaded_model.evaluate(test_batches, verbose=0)
#print("%s: %.2f%%" % (loaded_model.metrics_names[1], score[1]*100))

#!pip install sklearn
from sklearn.utils.fixes import sklearn
#!pip install flatten-dict
import numpy as np
#!python --version
from sklearn.utils import class_weight
#!sklearn --version
train_classes = train_batches.classes
class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(train_classes), y=train_classes)
#class_weights = class_weight.compute_class_weight('balanced', np.unique(train_batches.classes), train_batches.classes)
class_weights
#print('hola')
print(class_weights)
# train the model
#net_final.fit(train_batches,
#              validation_data = valid_batches,
#              epochs = 20,
#              steps_per_epoch= 1000,
#              class_weight=class_weights)
# load weights into new model
#class_weights= zip(np.unique(train_classes),class_weights)
#class_weights
print("Primer entrenament")
loaded_model.fit(train_batches,
                 validation_data = valid_batches,
                 steps_per_epoch=92,
                 epochs=10, 
                 class_weight = dict(enumerate(class_weights.flatten(), 0)),
				 #{0:115., 1:7.1875, 2:8.84615385, 3:0.14501892, 4:3.28571429, 5:57.5, 6:3.10810811, 7:5.}
#                 validation_data=valid_batches, 
#                 validation_steps=30,
                 #callbacks=[LearningRateScheduler]
                )

# save trained weights (% accuracy)
loaded_model.save("AM_mobilenetv2_disesasesclasses.h5")

#loaded_model.load_weights("AM_mobilenetv2_diseasesclasses.h5")
#print("Loaded model from disk")
#loaded_model.compile(optimizer=Adam(lr=5e-6), loss='categorical_crossentropy', metrics=['accuracy'])
#Re-train the model
#print(class_weights)
#print("Segon entrenament")
#loaded_model.fit(train_batches,
 #                validation_data = valid_batches,
 #                steps_per_epoch=60,
 #                epochs=10, 
 #                class_weight= dict(enumerate(class_weights.flatten(), 0)), 
 #                validation_data=valid_batches, 
 #                validation_steps=30,
 #             callbacks=[LearningRateScheduler]
 #               )

# save trained weights (% accuracy)
#loaded_model.save("AM_mobilenetv2_9classes_2.h5")

#def processfile(fname):
#    fname=fname[:-4]
#    splitting=fname.split('_')
#    filename=splitting[0][0:-len(splitting[1])]
#    print(filename)
#    return filename

#import numpy as np
#import statistics as st

#def computarpredict1(predi):
#    pred = np.argmax(predi, axis=1)
#    pred = st.mode(pred)    
    #print(pred)
#    return pred
    
#def computarpredict2(predi):
#    pred=predi.mean(0)
#    pred = np.argmax(pred, axis=0)
    #print(pred)
#    return pred

test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)
test_batches = test_datagen.flow_from_directory(r"E:\\TFG\\Raul\\final\\test\\",
                                                 classes=diseases_classes,
                                                 target_size=IM_SIZE,
                                                 class_mode='categorical', shuffle=False,
                                                 batch_size=4)

#print(np.shape(test_batches))

# show class indices
print('****************')
for cls, idx in test_batches.class_indices.items():
    print('Class nr ',idx,' -> ', cls)
print('****************')

print(test_batches.classes)

from keras import metrics

Y_pred = loaded_model.predict(test_batches)#, 74 // 7+1 #920 // 7+1)
#y_pred = np.argmax(Y_pred, axis=1)

# show filenames
#print('****************')
#fileN=test_batches.filenames
#print(fileN[0:4])
#for i in range(len(fileN)):
#    fileN[i]=processfile(fileN[i])
#print('****************')
#print(Y_pred[0:4])

#print(Y_pred[0:1,:])
#print(np.shape(Y_pred))
#print(np.shape(y_pred))

#cantos=[]
#real=[]
#pred1=[]
#pred2=[]
#previousIndex=0
#print(fileN[0:2])
#print('x: ', Y_pred)
#for i in range(1,len(fileN)):
#    if fileN[i]!=fileN[i-1]:
#        cantos.append((previousIndex, i-1))
#        real.append(test_batches.classes[i-1])
        #print(Y_pred[previousIndex:i-1,:])
#        if  Y_pred[previousIndex:i-1,:]!=[]:
#            pred1.append(computarpredict1(np.asarray(Y_pred[previousIndex:i,:])))
#            pred2.append(computarpredict2(np.asarray(Y_pred[previousIndex:i-1,:])))
#        else:
#            pred1.append(computarpredict1(np.asarray(Y_pred[previousIndex,:])))
#            pred2.append(computarpredict2(np.asarray(Y_pred[previousIndex,:])))

#        previousIndex=i

#print(cantos)
#print(pred1[0:3])

#correct1=0
#correct2=0
#incorrect1=0
#incorrect2=0
#for i in range(len(real)):
#    if real[i]==pred1[i]:
#        correct1=correct1+1
#    else:
#        incorrect1=incorrect1+1
        
#    if real[i]==pred2[i]:
#        correct2=correct2+1
#    else:
#        incorrect2=incorrect2+1
        
#print('Accuracy pred1:', correct1/(correct1+incorrect1))
#print('Accuracy pred2:', correct2/(correct2+incorrect2))

import numpy as np
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
y_pred = np.argmax(Y_pred, axis=1)

#plt.plot(test_batches, Y_pred)
#plt.show()
print(test_batches.classes)

print('Confusion Matrix')
print(confusion_matrix(test_batches.classes, y_pred))
print('Classification Report')
target_names=['Asthma',  'Bronchiectasis', 'Bronchiolitis', 'COPD', 'Healthy', 'LRTI', 'Pneumonia', 
              'URTI']

#target_names = ['0Alaud', '1Apusa', '2Cardu', '3Chlor', '4Cocco', '5Colum', '6Corvu', '7Delic', '8Dendr', '9Ember', 
#              '10Erith', '11Fring', '12Garru', '13Lusci', '14Motac', '15Parus', '16Passe', '17Passe', '18Phoen']
print(classification_report(test_batches.classes, y_pred, target_names=target_names))

