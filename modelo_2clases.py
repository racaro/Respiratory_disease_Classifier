# -*- coding: utf-8 -*-
"""MODELO_2Clases.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10rn0Z8yGJv47xUKwRiaxUz3tprgO2Qgm

### Convert Audios to Spectograms

#### 1.0 Import Modules
"""

from google.colab import drive
drive.mount('/content/drive')

path_base = "drive/MyDrive/PROYECTO/Clasificador_Enfermo/"

import os
import pandas as pd
import librosa
from librosa.display import specshow

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.layers import Dropout, Flatten, Dense 
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.preprocessing.image import ImageDataGenerator

import matplotlib.pyplot as plt
import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import sklearn.metrics

print(librosa.__version__)

path1 = path_base+"audio/train/sick"
path2 = path_base+"audio/train/not_sick"
path3 = path_base+"audio/validation/sick"
path4 = path_base+"audio/validation/not_sick"
carpeta_validation=path_base+"audio/validation"
carpeta_train=path_base+"audio/train"

directorio = os.listdir(path1)

path = path_base+"audio/train/"
num_sick=len(os.listdir(path1))
num_notsick=len(os.listdir(path2))
num_sick_val=len(os.listdir(path3))
num_notsick_val=len(os.listdir(path4))
total_train=num_notsick + num_sick
total_val=num_notsick_val+num_sick_val

"""#### 2.0 Preprocesamiento"""

directorio

def spec(path, directorio):
    for file in directorio:
        figure = plt.gcf()
        figure.set_size_inches(4, 3)
        archivo = path+"/"+file
        print(archivo)
        if ".wav" in file:
            #ffmpeg_extract_subclip(archivo, 0, 2, targetname=archivo)
            #sound = AudioSegment.from_wav(archivo)
            #sound.export(archivo,format="wav")
            scale, sr = librosa.load(archivo)
            stft = librosa.stft(scale)
            stft_db = librosa.amplitude_to_db(abs(stft))
            #mel_spectrogram = librosa.feature.melspectrogram(scale, sr, n_fft=2048, hop_length=512, n_mels=10, fmax=8000)
            #log_mel_spectrogram = librosa.power_to_db(mel_spectrogram)
            librosa.display.specshow(stft,sr=sr)
            print(archivo.split(".")[0])
            salida =archivo.split(".")[0] + ".jpg"
            plt.savefig(salida)

#path = "../audio/train/not_sick"
spec(path, directorio)

"""##### 2.1 Quitar .wav para cada carpeta"""

# train no sick
path = "../audio/train/not_sick"
spec(path, directorio)
for file in directorio:
    if ".wav" in file:
        src = os.path.join(path,file)
        os.remove(src)
    else:
        continue

# train sick
path = "../audio/train/sick"
spec(path, directorio)
for file in directorio:
    if ".wav" in file:
        src = os.path.join(path,file)
        os.remove(src)
    else:
        continue

# validation no sick
path "../audio/validation/not_sick"
spec(path, directorio)
for file in directorio:
    if ".wav" in file:
        src = os.path.join(path,file)
        os.remove(src)
    else:
        continue

# validation sick
path "../audio/validation/sick"
spec(path, directorio)
for file in directorio:
    if ".wav" in file:
        src = os.path.join(path,file)
        os.remove(src)
    else:
        continue

# test no sick
path "../audio/test/not_sick"
spec(path, directorio)
for file in directorio:
    if ".wav" in file:
        src = os.path.join(path,file)
        os.remove(src)
    else:
        continue

# test sick
path "../audio/test/sick"
spec(path, directorio)
for file in directorio:
    if ".wav" in file:
        src = os.path.join(path,file)
        os.remove(src)
    else:
        continue

"""### 3.0 Modelo

#### 3.1 Modelo pequeño
"""

path

import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import sklearn.metrics

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.layers import Dropout, Flatten, Dense 
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(
    rescale = 1./255,
    shear_range = 0.2,
    zoom_range = 0.2,
    validation_split=0.2
)

train_generator = train_datagen.flow_from_directory(
    path,
    target_size = (192,192),
    color_mode = "rgb",
    batch_size = 32,
    class_mode = "binary",
    subset="training",
    shuffle=True
)

validation_generator = train_datagen.flow_from_directory(
    path,
    target_size = (192,192),
    batch_size = 32,
    class_mode = "binary",
    subset="validation",
    shuffle= True
)

model = Sequential()

model.add(Conv2D(192,(3,3), input_shape=(192,192,3), activation="relu"))
model.add(MaxPooling2D(pool_size=(2,2)))

model.add(Conv2D(64,(3,3), activation="relu"))
model.add(MaxPooling2D(pool_size=(2,2)))

model.add(Conv2D(64,(3,3), activation="relu"))
model.add(Dropout(0.5))

model.add(Flatten())
model.add(Dense(64,activation="relu"))
model.add(Dense(10,activation="sigmoid"))

model.compile(loss= "binary_crossentropy", optimizer= "rmsprop", metrics=["accuracy"])
model.fit(train_generator,validation_data= validation_generator,epochs=10)

model = Sequential([

Conv2D(192,(3,3), input_shape=(192,192,3), activation="relu"),
MaxPooling2D(pool_size=(2,2)),
Conv2D(64,(3,3), activation="relu"),
MaxPooling2D(pool_size=(2,2)),
Conv2D(64,(3,3), activation="relu"),
Dense(10, activation='sigmoid'),
Flatten(),
Dense(64,activation="relu"),
Dense(10,activation="sigmoid"),
Dense(10, activation='softmax')
])

model.compile(loss= "binary_crossentropy", optimizer= "rmsprop", metrics=["accuracy"])
model.fit(train_generator,validation_data= validation_generator,epochs=10)

model.save("drive/MyDrive/PROYECTO/modelos/modelo_enfermo.h5")

"""##### 3.12 Metricas"""

model.evaluate(validation_generator)

y_pred = model.predict(validation_generator)

y_pred = y_pred.argmax(axis=1)

validation_generator.classes

transformador =train_generator.class_indices

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt

###2.0 Metricas

from sklearn.metrics import auc,roc_curve,confusion_matrix,ConfusionMatrixDisplay

cm = confusion_matrix(validation_generator.classes,y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(transformador.values()))
disp.plot()

from sklearn.metrics import f1_score
print(f1_score(validation_generator.classes, y_pred , average="macro"))

"""#### 3.2 Modelo Grande"""

train_datagen = ImageDataGenerator(
    validation_split=0.2,
    rescale=1./255,
    rotation_range=40,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

from tensorflow.python.ops.gen_math_ops import TruncateDiv
train_generator = train_datagen.flow_from_directory(
    path,
    target_size = (288,216),
    color_mode = "rgb",
    batch_size = 32,
    class_mode = "binary",
    subset="training",
    shuffle=True
)

validation_generator = train_datagen.flow_from_directory(
    path,
    target_size = (288,216),
    batch_size = 32,
    class_mode = "binary",
    subset="validation",
    shuffle=True
)
TAMANO_LOTE=150

model = Sequential()

model.add(Conv2D(32,(3,3), input_shape=(288,216,3), activation="relu"))
model.add(MaxPooling2D(pool_size=(2,2)))

model.add(Conv2D(64,(3,3), activation="relu"))
model.add(MaxPooling2D(pool_size=(2,2)))

model.add(Conv2D(128,(3,3), activation="relu"))
model.add(MaxPooling2D(pool_size=(2,2)))

model.add(Conv2D(128,(3,3), activation="relu"))
model.add(MaxPooling2D(pool_size=(2,2)))

#model.add(Conv2D(24,(3,3), activation="relu"))
#model.add(MaxPooling2D(pool_size=(2,2)))

#model.add(Conv2D(16,(3,3), activation="relu"))
#model.add(MaxPooling2D(pool_size=(2,2)))

model.add(Dropout(0.5))
model.add(Flatten())
model.add(Dense(64,activation="relu"))
#model.add(Dense(32,activation="relu"))
model.add(Dense(512,activation="relu"))
model.add(Dense(1,activation="sigmoid"))

model.compile(loss= "binary_crossentropy", optimizer= "rmsprop", metrics=["accuracy"])
historial=model.fit(train_generator,validation_data= validation_generator,epochs=60)
#model.fit(train_generator,validation_data= validation_generator,epochs=60,validation_steps=int(np.ceil(total_val / float(TAMANO_LOTE))),steps_per_epoch=int(np.ceil(total_train / float(TAMANO_LOTE))))

model.save("drive/MyDrive/PROYECTO/modelos/modelo_enfermo.h5")

model.evaluate(validation_generator)

"""##### 3.12 Metricas"""

model.evaluate(validation_generator)

y_pred = model.predict(validation_generator)

y_pred = y_pred.argmax(axis=1)

validation_generator.classes

transformador =train_generator.class_indices

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt

###2.0 Metricas

from sklearn.metrics import auc,roc_curve,confusion_matrix,ConfusionMatrixDisplay

cm = confusion_matrix(validation_generator.classes,y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(transformador.values()))
disp.plot()

from sklearn.metrics import f1_score
print(f1_score(validation_generator.classes, y_pred , average="macro"))

"""##### 3.13 Guardar modelo"""

path

model.save("drive/MyDrive/PROYECTO/Clasificador_Enfermo/modelo_grande_enfermo.h5")

"""### 4.0 Hyperparameter Tuning"""

!pip install -q -U keras-tuner

import keras_tuner as kt

train_datagen = ImageDataGenerator(
    rescale = 1./255,
    rotation_range = 40,
    shear_range = 0.2,
    zoom_range = 0.2,
    validation_split = 0.2,
    horizontal_flip = True,
    fill_mode = 'nearest',
    height_shift_range = 0.2
)

path

from tensorflow.python.ops.gen_math_ops import TruncateDiv
train_generator = train_datagen.flow_from_directory(
    path,
    target_size = (288,216),
    color_mode = "rgb",
    batch_size = 32,
    class_mode = "binary",
    subset="training",
    shuffle=True
)

validation_generator = train_datagen.flow_from_directory(
    path,
    target_size = (288,216),
    batch_size = 32,
    class_mode = "binary",
    subset="validation",
    shuffle=True
)

from operator import le

def model_builder(hp):
  model = Sequential()


  #Cambiamos numero de filtros 
  hp_numero_nodos_primera = hp.Int("units",min_value=64,max_value=128,step=16)

  model.add(Conv2D(hp_numero_nodos_primera,(3,3), input_shape=(288,216,3), activation="relu"))
  model.add(MaxPooling2D(pool_size=(2,2)))

  hp_numero_nodos_segunda = hp.Int("units",min_value=32,max_value=64,step=16)
  model.add(Conv2D(hp_numero_nodos_segunda,(3,3), activation="relu"))
  model.add(MaxPooling2D(pool_size=(2,2)))

  hp_numero_nodos_tercera = hp.Int("units",min_value=16,max_value=32,step=8)
  model.add(Conv2D(hp_numero_nodos_tercera,(3,3), activation="relu"))
  model.add(MaxPooling2D(pool_size=(2,2)))

  hp_numero_nodos_cuarta = hp.Int("units",min_value=8,max_value=16,step=4)
  model.add(Conv2D(hp_numero_nodos_cuarta,(3,3), activation="relu"))
  model.add(MaxPooling2D(pool_size=(2,2)))

  #
  model.add(Dropout(0.5))
  model.add(Flatten())
  hp_numero_nodos_quinta = hp.Int("units",min_value=64,max_value=128,step=16)
  model.add(Dense(512,activation="relu"))
  model.add(Dense(1,activation="sigmoid"))

  lr = hp.Choice("learning_rate",values=[1e-1,1e-2,1e-3,1e-4])

  model.compile(loss= "binary_crossentropy", optimizer= tf.keras.RMSprop(lr=le-4), metrics=["accuracy"])
  return model
  batch_size = 100
  steps_per_epoch=train_generator.n // batch_size
  validation_steps=validation_steps
  history=model.fit(train_generator,steps_per_epoch=steps_per_epoch,epochs=100,validation_data= validation_generator,validation_steps=validation_steps,verbose=2)

batch_size = 100
  steps_per_epoch=train_generator.n // batch_size
  validation_steps=validation_steps
  history=model.fit(train_generator,steps_per_epoch=steps_per_epoch,epochs=100,validation_data= validation_generator,validation_steps=validation_steps,verbose=2)

'''
tuner = kt.Hyperband(model_builder,
                     objective="accuracy",
                     max_epochs=8,
                     directory="my_dir",
                     project_name="proyectoo")
                     '''

#stop_early = tf.keras.callbacks.EarlyStopping(monitor="accuracy",patience=5)

'''
tuner.search(train_generator,epochs=8,callbacks=[stop_early],validation_data=validation_generator)
best_model = tuner.get_best_models()[0]
'''

best_model = tuner.get_best_models()[0]

best_model.save("drive/MyDrive/PROYECTO/modelos/modelo_enfermo_hp.h5")



"""### 5.0 Real results"""

#Diagrama
    # Tabla Audios
    # Tabla imagenes
    # Resultados

import tensorflow
from tensorflow.keras.models import load_model

model = load_model("drive/MyDrive/PROYECTO/modelos/modelo_enfermo_hp.h5")

#Accuracy de modelo
model.evaluate(validation_generator)

#Prediccion de datos de validación
y_pred = model.predict(validation_generator)

#Selecciona las clases
y_pred = y_pred.argmax(axis=1)

y_pred

#Visualizamos predicción
validation_generator.classes

transformador =train_generator.class_indices

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt

###2.0 Metricas

from sklearn.metrics import auc,roc_curve,confusion_matrix,ConfusionMatrixDisplay
#Matri< de confusion
cm = confusion_matrix(validation_generator.classes,y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=list(transformador.values()))
disp.plot()

from sklearn.metrics import f1_score

#Score F1, metrica mas completa que el accuracy, mientras mas alto mejor
print(f1_score(validation_generator.classes, y_pred , average="macro"))